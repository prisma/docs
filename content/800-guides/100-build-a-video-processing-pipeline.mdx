---
title: 'How to build a video processing pipeline'
metaTitle: 'How to build a video processing pipeline with Prisma Pulse and Trigger.dev'
description: 'Learn how to build a scalable video processing pipeline using a decoupled, event-driven architecture with Prisma Pulse and Trigger.dev.'
sidebar_label: 'Video processing pipeline'
image: '/img/guides/video-processing-pipeline-cover.svg'
tags:
  - video-processing
  - pulse
  - trigger.dev
  - automation
  - serverless
  - event-driven
  - workflows
  - real-time
  - scalability
  - integration
---

## Introduction

Serverless computing enables applications to scale efficiently, supporting millions of users. However, it faces challenges with longer runtimes and intensive data processing, both of which are crucial for machine learning (ML) applications. This guide will show you how to use Pulse and Trigger.dev to create decoupled, event-driven workflows for efficient handling of complex video processing tasks.

## Prerequisites

Before starting this guide, make sure you have:

- Node.js installed (version 18 or higher)
- A PostgreSQL database
- A [Trigger.dev](https://trigger.dev) account
- A [Deepgram](https://deepgram.com) API key for transcription
- FFmpeg installed on your system

## 1. Set up your project

### 1.1. Create a new project

First, create a new Node.js project and install the necessary dependencies:

```bash
mkdir video-processing-pipeline
cd video-processing-pipeline
npm init -y
npm install @prisma/client @trigger.dev/sdk @prisma/pulse deepgram fluent-ffmpeg
npm install prisma --save-dev
```

### 1.2. Define the data model

Initialize Prisma and create your data model:

```bash
npx prisma init
```

Add the following model to your `schema.prisma`:

```prisma
model Video {
  id             Int      @id @default(autoincrement())
  url            String
  transcription  String?
  createdAt      DateTime @default(now())
  updatedAt      DateTime @updatedAt
}
```

### 1.3. Run database migration

Apply the database schema:

```bash
npx prisma migrate dev --name init
```

## 2. Implement the video processing pipeline

### 2.1. Set up Trigger.dev workflow

Next, we'll set up a transcription task using Trigger.dev. This script will take a video URL, extract its audio, and transcribe it using the Deepgram API:

Create a new file called `transcription-workflow.ts`:

```typescript
import { Job, Trigger } from '@trigger.dev/sdk';
import { prisma } from './prisma';
import { deepgram } from './deepgram';
import ffmpeg from 'fluent-ffmpeg';
import { Readable } from 'stream';

new Trigger({
  id: 'video-transcription',
  name: 'Video Transcription',
  on: 'video.uploaded',
  run: async (event) => {
    const video = await prisma.video.findUnique({
      where: { id: event.videoId },
    });

    if (!video) {
      throw new Error('Video not found');
    }

    const audioStream = new Readable();
    ffmpeg(video.url)
      .noVideo()
      .audioCodec('pcm_s16le')
      .format('wav')
      .pipe(audioStream);

    const transcription = await deepgram.transcription.preRecorded(
      { buffer: audioStream },
      { punctuate: true }
    );

    await prisma.video.update({
      where: { id: video.id },
      data: { transcription: transcription.results.channels[0].alternatives[0].transcript },
    });
  },
});
```

### 2.2. Configure Prisma Pulse events

You can now create a file `index.ts` that will act as an API endpoint for uploading videos. When a new video is uploaded, we use Prisma ORM to save the video URL in the database:

```typescript
import { prisma } from './prisma';

async function uploadVideo(url: string) {
  const video = await prisma.video.create({
    data: { url },
  });

  // Emit 'video.uploaded' event
  // ...
}
```

Now, to connect Pulse and trigger.dev, create a new file called `pulse-events.ts`:

```typescript
import { pulse } from '@prisma/pulse';
import { trigger } from './trigger';

pulse.model('Video').on('create', async (video) => {
  await trigger.emit('video.uploaded', { videoId: video.id });
});

pulse.model('Video').on('update', async (video) => {
  if (video.transcription) {
    console.log('Transcription completed:', video.id);
    // Notify a client or trigger additional actions here
  }
});
```

## Next steps

Now that you have a working video processing pipeline, you can:

- Add error handling and retries
- Implement progress tracking
- Add support for different video formats
- Scale your pipeline with additional processing steps

For more information and updates:
- [Get started with Trigger.dev](https://trigger.dev)
- [Get started with Pulse](https://www.prisma.io/pulse)
- Join our [Discord community](https://pris.ly/discord)
